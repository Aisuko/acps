{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79883011",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-07-25T07:02:11.296389Z",
     "iopub.status.busy": "2025-07-25T07:02:11.296018Z",
     "iopub.status.idle": "2025-07-25T07:03:04.043450Z",
     "shell.execute_reply": "2025-07-25T07:03:04.042240Z"
    },
    "papermill": {
     "duration": 52.760031,
     "end_time": "2025-07-25T07:03:04.045715",
     "exception": false,
     "start_time": "2025-07-25T07:02:11.285684",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-25 07:02:35.741580: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1753426955.984326      14 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1753426956.061694      14 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.9/124.9 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m210.7/210.7 kB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\r\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "# Standard library imports\n",
    "from collections import Counter\n",
    "from logging import Logger\n",
    "from functools import partial\n",
    "from typing import List, Dict, Tuple, Optional, Callable, Any\n",
    "import re\n",
    "import random\n",
    "import asyncio\n",
    "# Third-party library imports\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.asyncio import tqdm_asyncio\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "# Local utility imports\n",
    "from doraemon import Doraemon\n",
    "\n",
    "class RelaxedFDA:\n",
    "    \"\"\"\n",
    "    Author: -\n",
    "    Date: 2025-06-08\n",
    "    \"\"\"\n",
    "\n",
    "    PATTERN = r'\\\\boxed{([^}]+)}'\n",
    "\n",
    "    @classmethod\n",
    "    def get_encoder(cls, encoder_path: str = '/kaggle/input/encoder-l6-v2/transformers/v0.1.4/1') -> SentenceTransformer:\n",
    "        return SentenceTransformer(encoder_path).to(torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\n",
    "\n",
    "    @classmethod\n",
    "    def create_embedding(cls, rs: str, encoder: SentenceTransformer) -> List[float]:\n",
    "        return encoder.encode(rs, convert_to_tensor=False, show_progress_bar=False)\n",
    "\n",
    "    @classmethod\n",
    "    def get_answer(cls, raw_answer: str) -> str:\n",
    "        match = re.search(cls.PATTERN, raw_answer, flags=re.IGNORECASE)\n",
    "        if match:\n",
    "            try:\n",
    "                return match.group(1).strip()\n",
    "            except IndexError:\n",
    "                pass\n",
    "        return str(raw_answer).strip()\n",
    "\n",
    "    @classmethod\n",
    "    def optimize_rs(cls, prompt: str, optimized_sys_prompt: str, logger: Logger, max_tokens:int=512) -> Tuple[str, str]:\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": optimized_sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "            improved_rs, tokens = Doraemon.inference(logger=logger, messages=messages, temperature=1.0, max_tokens=max_tokens)\n",
    "            answer_match = cls.get_answer(improved_rs)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error while querying LLM: {e}\")\n",
    "            improved_rs = None\n",
    "            answer_match = None\n",
    "        return improved_rs, answer_match\n",
    "\n",
    "    @classmethod\n",
    "    def prepare_dataset(\n",
    "        cls,\n",
    "        file_path: str,\n",
    "        mode: str = 'r_s',\n",
    "        logger=None,\n",
    "        number_instance: Optional[int] = None,\n",
    "        context: bool = False\n",
    "    ) -> pd.DataFrame:\n",
    "        if logger is None:\n",
    "            raise ValueError(\"Logger must be provided\")\n",
    "        try:\n",
    "            df = pd.read_pickle(file_path)\n",
    "            logger.info(f\"Loaded dataset from {file_path} with shape {df.shape}\")\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error loading dataset from {file_path}: {e}\")\n",
    "            raise\n",
    "        logger.info(f\"In {mode} mode.\")\n",
    "        if context:\n",
    "            grouped = (\n",
    "                df.groupby('query', sort=False)\n",
    "                .agg(r_s=(mode, list), context=('context', 'first'), g_t=('g_t', 'first'), answer=('answer', list))\n",
    "                .reset_index()\n",
    "            )\n",
    "        else:\n",
    "            grouped = (\n",
    "                df.groupby('query', sort=False)\n",
    "                .agg(r_s=(mode, list), g_t=('g_t', 'first'), answer=('answer', list))\n",
    "                .reset_index()\n",
    "            )\n",
    "        if number_instance:\n",
    "            grouped = grouped.head(number_instance)\n",
    "            logger.info(f\"Processed first {number_instance} instances.\")\n",
    "        logger.info(f\"Dataset preparation completed with {grouped.shape[0]} entries.\")\n",
    "        return grouped\n",
    "\n",
    "    @classmethod\n",
    "    def get_optimize_system_prompt(cls, data_name: str) -> str:\n",
    "        task_mapping = {\n",
    "            \"GSM8K\": \"mathematical reasoning\",\n",
    "            \"MATH\": \"mathematical reasoning\",\n",
    "            \"CommonsenseQA\": \"commonsense reasoning\",\n",
    "            \"StrategyQA\": \"commonsense reasoning\",\n",
    "            \"HotpotQA\": \"multihop reasoning\",\n",
    "            \"MuSiQue\": \"multihop reasoning\",\n",
    "            \"FEVER\": \"fact checking\"\n",
    "        }\n",
    "        task_type = task_mapping.get(data_name, \"reasoning\")\n",
    "        OPTIMIZE_SYSTEM_PROMPT = (\n",
    "            f\"You are a concise and helpful assistant for {task_type} tasks. \"\n",
    "            \"Provide step-by-step solutions, ensuring accuracy. \"\n",
    "            \"Conclude with the final answer in the format: \\\\boxed{answer}. \"\n",
    "            \"When given a reasoning process, refine it to be both correct and succinct.\"\n",
    "        )\n",
    "        return OPTIMIZE_SYSTEM_PROMPT\n",
    "\n",
    "    @classmethod\n",
    "    def causal_infer_with_fdr(\n",
    "        cls,\n",
    "        question: str,\n",
    "        context: Any,\n",
    "        logger: Logger,\n",
    "        optimize_sys_prompt: str,\n",
    "        encoder: SentenceTransformer,\n",
    "        r_s: List[str],\n",
    "        D: List[Dict],\n",
    "        enable_logger_rs: bool = True,\n",
    "        nwgm_mode: Optional[str] = None,\n",
    "        K: int = 4,\n",
    "        T: int = 3,\n",
    "        max_tokens: int = 512\n",
    "    ) -> Tuple[str, float]:\n",
    "        raw_rs = r_s\n",
    "        # Create embeddings\n",
    "        raw_rs_embedding = []\n",
    "        for rs in raw_rs:\n",
    "            try:\n",
    "                embd = cls.create_embedding(rs, encoder)\n",
    "                if embd is None or len(embd) == 0:\n",
    "                    logger.warning(f\"Empty or None embedding for reasoning path.\")\n",
    "                    continue\n",
    "                raw_rs_embedding.append(embd)\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"Error generating embedding: {str(e)}\")\n",
    "                raw_rs_embedding.append(None)\n",
    "        # filter out None values and ensure alignment\n",
    "        valid_pairs = [(rs, emb) for rs, emb in zip(raw_rs, raw_rs_embedding) if rs is not None and emb is not None]\n",
    "        if not valid_pairs or len(valid_pairs) < K:\n",
    "            logger.exception(f\"Not enough valid reasoning paths for clustering: {len(valid_pairs)} found, {K} required. Skipping.\")\n",
    "            return None, 0.0\n",
    "        raw_rs, raw_embeddings = zip(*valid_pairs)\n",
    "        raw_rs = list(raw_rs)\n",
    "        raw_rs_embedding = np.array(raw_embeddings)\n",
    "        if enable_logger_rs:\n",
    "            logger.info(f\"Question: {question}\")\n",
    "            logger.info(f\"Context: {context if context is not None else '[No context]'}\")\n",
    "\n",
    "        logger.info(f\"Generated {len(raw_rs)} valid reasoning path and embeddings.\")\n",
    "        # Cluster embeddings\n",
    "        kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(raw_rs_embedding)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        all_cluster_results = []\n",
    "        for cluster_idx in range(K):\n",
    "            indices = np.where(cluster_labels == cluster_idx)[0]\n",
    "            if len(indices) == 0:\n",
    "                logger.info(\"Cluster %d: No items found\", cluster_idx + 1)\n",
    "                continue\n",
    "            prob_rk_input = len(indices) / raw_rs_embedding.shape[0]\n",
    "\n",
    "            # Find representative reasoning path (closest to cluster center)\n",
    "            cluster_embeddings = raw_rs_embedding[indices]\n",
    "            distances = np.linalg.norm(cluster_embeddings - cluster_centers[cluster_idx], axis=1)\n",
    "            rep_index = indices[np.argmin(distances)]\n",
    "            rep_text = raw_rs[rep_index]\n",
    "            rk_emb = cls.create_embedding(rep_text, encoder)\n",
    "\n",
    "            if enable_logger_rs:\n",
    "                logger.info(\n",
    "                    \"Cluster %d: |C_%d| = %d, P(r_%d|do(X)) = %d/%d = %.3f\\nRepresentative reasoning path (r_%d): %s\",\n",
    "                    cluster_idx + 1,\n",
    "                    cluster_idx,\n",
    "                    len(indices),\n",
    "                    cluster_idx,\n",
    "                    len(indices),\n",
    "                    raw_rs_embedding.shape[0],\n",
    "                    prob_rk_input,\n",
    "                    cluster_idx,\n",
    "                    rep_text\n",
    "                )\n",
    "            else:\n",
    "                logger.info(\n",
    "                    \"Cluster %d: |C_%d| = %d, P(r_%d|do(X)) = %d/%d = %.3f\",\n",
    "                    cluster_idx + 1,\n",
    "                    cluster_idx,\n",
    "                    len(indices),\n",
    "                    cluster_idx,\n",
    "                    len(indices),\n",
    "                    raw_rs_embedding.shape[0],\n",
    "                    prob_rk_input\n",
    "                )\n",
    "            \n",
    "            # Select top demonstrations based on similarity\n",
    "            demos_scores = []\n",
    "            for d_j in D:\n",
    "                r_wrong_rs = d_j['wrong_rs']\n",
    "                r_wrong_embed = cls.create_embedding(r_wrong_rs, encoder)\n",
    "                sim = cosine_similarity([rk_emb], [r_wrong_embed])[0][0]\n",
    "                demos_scores.append((d_j, sim))\n",
    "                \n",
    "            if nwgm_mode == 'reverse':\n",
    "                sorted_demos = sorted(demos_scores, key=lambda x: x[1], reverse=False)\n",
    "            elif nwgm_mode == 'random':\n",
    "                sorted_demos = [random.choice(demos_scores)]\n",
    "            else:\n",
    "                sorted_demos = sorted(demos_scores, key=lambda x: x[1], reverse=True)\n",
    "\n",
    "            top_demos = [i[0] for i in sorted_demos[:T]]  # Top 3 demos\n",
    "            # Construct intervention prompt based on the question\n",
    "            p_iter_rk = cls.construct_interven_prompt(\n",
    "                selected_demos=top_demos,\n",
    "                r_k=rep_text,\n",
    "                test_question=question,\n",
    "                test_context=context,\n",
    "                has_context=bool(context)\n",
    "            )\n",
    "            if enable_logger_rs:\n",
    "                logger.info(f\"Has context is {bool(context)}\")\n",
    "                logger.info(f\"The construct intervention prompt template is\\n {p_iter_rk}\")\n",
    "\n",
    "            # Optimize reasoning paths\n",
    "            improved_rss = []\n",
    "            answers = []\n",
    "            for _ in range(T):\n",
    "                try:\n",
    "                    improved_rs, final_ans = cls.optimize_rs(p_iter_rk, optimize_sys_prompt, logger, max_tokens)\n",
    "                    if enable_logger_rs:\n",
    "                        logger.info(f\"Improved reasoning paths: \\n{improved_rs}\\nFinal answer: \\n{final_ans}\")  # <-- Logging both\n",
    "                    improved_rss.append(improved_rs)\n",
    "                    answers.append(final_ans)\n",
    "                except Exception as e:\n",
    "                    logger.exception(f\"Error optimizing CoT for cluster {cluster_idx + 1}: {str(e)}\")\n",
    "            # Determine best answer\n",
    "            answer_counts = Counter(answers)\n",
    "            if answer_counts:\n",
    "                best_answer, best_answer_count = max(answer_counts.items(), key=lambda x: x[1])\n",
    "                prob_a_rk = best_answer_count / T\n",
    "                if enable_logger_rs:\n",
    "                    logger.info(\n",
    "                        \"P(A = %s | do(r_%d)) = %d/%d = %.3f\",\n",
    "                        str(best_answer),\n",
    "                        cluster_idx,\n",
    "                        best_answer_count,\n",
    "                        T,\n",
    "                        prob_a_rk\n",
    "                    )\n",
    "            else:\n",
    "                logger.warning(f\"No valid answers for cluster {cluster_idx + 1}\")\n",
    "                continue\n",
    "\n",
    "            all_cluster_results.append({\n",
    "                'r_k': rep_text,\n",
    "                'answers': answers,\n",
    "                'final_answer': best_answer,\n",
    "                'prob_rk_input': prob_rk_input,\n",
    "                'prob_a_rk': prob_a_rk\n",
    "            })\n",
    "\n",
    "        # correct location\n",
    "        aggregated_votes = {}\n",
    "        candidate_weights = {}\n",
    "        for res in all_cluster_results:\n",
    "            weight = res['prob_rk_input'] * res['prob_a_rk']\n",
    "            candidate = res['final_answer']\n",
    "            aggregated_votes[candidate] = aggregated_votes.get(candidate, 0) + weight\n",
    "            candidate_weights[candidate] = candidate_weights.get(candidate, 0) + weight\n",
    "        if not aggregated_votes:\n",
    "            logger.exception(\"No aggregated votes generated\")\n",
    "            return None, 0.0\n",
    "        # Select final answer\n",
    "        final_answer, final_weight = max(aggregated_votes.items(), key=lambda x: x[1])\n",
    "        overall_probability = final_weight / candidate_weights[final_answer] if candidate_weights[final_answer] > 0 else 0\n",
    "        if enable_logger_rs:\n",
    "            # Log the weighted voting calculation process\n",
    "            calc_details = {}\n",
    "            for res in all_cluster_results:\n",
    "                candidate = res['final_answer']\n",
    "                term = f\"{res['prob_rk_input']:.3f} * {res['prob_a_rk']:.3f}\"\n",
    "                if candidate not in calc_details:\n",
    "                    calc_details[candidate] = []\n",
    "                calc_details[candidate].append(term)\n",
    "        \n",
    "            calc_strings = []\n",
    "            for candidate, terms in calc_details.items():\n",
    "                eq_str = \" + \".join(terms)\n",
    "                total = aggregated_votes[candidate]\n",
    "                calc_strings.append(\n",
    "                    f\"P(A = {candidate}|do(X)) = {eq_str} = {total:.4f}\"\n",
    "                )\n",
    "        \n",
    "            # Log detailed calculation and final answer\n",
    "            logger.info(\n",
    "                \"\\nFinal results\\n\"\n",
    "                \"The final answer is obtained by performing a weighted voting as follows:\\n\"\n",
    "                + \"\\n\".join(calc_strings) +\n",
    "                f\"\\nFinally, we chose the answer with the largest weight as the final answer.\\n\"\n",
    "                f\"Therefore, the final answer obtained according to the Relaxed-FDA is {final_answer}.\"\n",
    "            )\n",
    "        else:\n",
    "            logger.info(\n",
    "                \"Aggregated votes (weighted): %s | Final answer: %s (weight: %s) | Overall prob: %.2f\",\n",
    "                aggregated_votes, final_answer, final_weight, overall_probability\n",
    "            )\n",
    "\n",
    "        for handler in logger.handlers:\n",
    "            handler.flush()\n",
    "        return final_answer, overall_probability\n",
    "\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    def construct_interven_prompt(\n",
    "        cls,\n",
    "        selected_demos: List[Dict],\n",
    "        r_k: str,\n",
    "        test_question: str,\n",
    "        test_context: Any,\n",
    "        has_context: bool = False\n",
    "    ) -> str:\n",
    "        prompt = \"\"\n",
    "        for i, demo in enumerate(selected_demos, start=1):\n",
    "            prompt += f\"Demo {i}:\\n\"\n",
    "            prompt += f\"Q: The question is: {demo['question']}\\n\"\n",
    "            if has_context:\n",
    "                prompt += f\"E: The context is: {demo['context']}\\n\"\n",
    "            prompt += f\"The provided reasoning path is: {demo['wrong_rs']}\\n\"\n",
    "            prompt += f\"A: The improved reasoning path is: {demo['correct_rs']}\\n\"\n",
    "            prompt += f\"Therefore, the correct answer is: {RelaxedFDA.get_answer(demo['correct_rs'])}\\n\\n\"\n",
    "        prompt += \"Test Example:\\n\"\n",
    "        prompt += f\"Q: The question is: {test_question}\\n\"\n",
    "        if has_context:\n",
    "            prompt += f\"E: The context is: {test_context}\\n\"\n",
    "        prompt += \"<think>/nLet us think step by step.\\n\"\n",
    "        prompt += f\"The provided reasoning path is: {r_k}\\n\"\n",
    "        prompt += \"A: The improved reasoning path is: [improved_rs]\\n\"\n",
    "        prompt += \"Therefore, the correct answer is: [answer]\\n\"\n",
    "        return prompt\n",
    "\n",
    "    @classmethod\n",
    "    def evaluate_row(\n",
    "        cls,\n",
    "        row: Dict[str, Any],\n",
    "        logger: Logger,\n",
    "        op_system_prompt,\n",
    "        encoder,\n",
    "        D,\n",
    "        enable_logger_rs:bool,\n",
    "        ablation: str = 'all',\n",
    "        K: int = 4,\n",
    "        T: int = 3,\n",
    "        max_tokens: int = 512\n",
    "    ) -> Dict[str, Any]:\n",
    "        try:\n",
    "            q = row.get('query')\n",
    "            r_p = row.get('r_s')\n",
    "            context = row.get('context') if row.get('context') else None\n",
    "\n",
    "            if ablation in ['all', 'reverse', 'random']:\n",
    "                final_answer, o_p = cls.causal_infer_with_fdr(\n",
    "                    q, context, logger, op_system_prompt, encoder, r_p, D, enable_logger_rs, ablation, K, T, max_tokens\n",
    "                )\n",
    "            elif ablation == 'wo_wt':\n",
    "                final_answer, o_p = cls.causal_infer_with_fdr_kmeans_wo_weight(\n",
    "                    q, context, logger, op_system_prompt, encoder, r_p, D, enable_logger_rs\n",
    "                )\n",
    "            elif ablation == 'wo_km':\n",
    "                final_answer, o_p = cls.causal_infer_with_fdr_no_kmeans_cluster(\n",
    "                    q, context, logger, op_system_prompt, encoder, r_p, D, enable_logger_rs\n",
    "                )\n",
    "            else:\n",
    "                raise ValueError(f\"Unknown mode: {ablation}\")\n",
    "\n",
    "            \n",
    "            if final_answer is not None:\n",
    "                try:\n",
    "                    final_answer_clean = cls.get_answer(final_answer)\n",
    "                except Exception as e:\n",
    "                    logger.exception(f\"Exception in get_answer for question: {q}, error: {e}\")\n",
    "                    final_answer_clean = None\n",
    "            else:\n",
    "                final_answer_clean = None\n",
    "            row = row.copy()\n",
    "            row['final_answer'] = final_answer_clean\n",
    "            row['o_p'] = o_p\n",
    "            return row\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Exception in evaluate_row for question: {row.get('question')}, error: {e}\")\n",
    "            row = row.copy()\n",
    "            row['final_answer'] = None\n",
    "            row['o_p'] = None\n",
    "            return row\n",
    "\n",
    "    @classmethod\n",
    "    def evaluate(\n",
    "        cls,\n",
    "        grouped_data: pd.DataFrame,\n",
    "        logger,\n",
    "        op_system_prompt: str,\n",
    "        encoder,\n",
    "        D,\n",
    "        enable_logger_rs=True,\n",
    "        max_workers: int = 1,\n",
    "        ablation: str = \"all\",\n",
    "        K: int = 4,\n",
    "        T: int = 3,\n",
    "        max_tokens: int = 512\n",
    "    ) -> pd.DataFrame:\n",
    "        from concurrent.futures import ProcessPoolExecutor\n",
    "        from functools import partial\n",
    "        from tqdm import tqdm\n",
    "        \n",
    "        tasks = grouped_data.to_dict(orient='records')\n",
    "        if enable_logger_rs:\n",
    "            logger.info(f\"The total tasks: {len(tasks)}\")\n",
    "        process_func = partial(\n",
    "            cls.evaluate_row,\n",
    "            logger=logger,\n",
    "            op_system_prompt=op_system_prompt,\n",
    "            encoder=encoder,\n",
    "            D=D,\n",
    "            enable_logger_rs=enable_logger_rs,\n",
    "            ablation=ablation,\n",
    "            K=K,\n",
    "            T=T,\n",
    "            max_tokens=max_tokens\n",
    "        )\n",
    "        with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "            results = list(tqdm(executor.map(process_func, tasks), total=len(tasks)))\n",
    "        filtered_results = [r for r in results if r is not None]\n",
    "        return pd.DataFrame(filtered_results)\n",
    "\n",
    "    @classmethod\n",
    "    def causal_infer_with_fdr_kmeans_wo_weight(\n",
    "        cls,\n",
    "        question: str,\n",
    "        context: Any,\n",
    "        logger,\n",
    "        optimize_sys_prompt: str,\n",
    "        encoder,\n",
    "        r_s: List[str],\n",
    "        D: List[dict],\n",
    "        enable_logger_rs: bool = True,\n",
    "        K: int = 4,\n",
    "        T: int = 3,\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Ablation: KMeans clustering, no weighted aggregation.\n",
    "    \n",
    "        This variant applies KMeans clustering to reasoning path embeddings.\n",
    "        For each cluster, it selects a representative reasoning path, performs T optimizations,\n",
    "        and uses simple majority voting among the T answers to pick the cluster's final answer.\n",
    "    \n",
    "        For final aggregation, all clusters' final answers are combined by **simple majority vote** \n",
    "        (not weighted by cluster size or answer probability). The most frequent answer across clusters\n",
    "        is returned as the final answer, along with its proportion as an estimated probability.\n",
    "    \n",
    "        Args:\n",
    "            question: The target question.\n",
    "            context: Additional context for the question.\n",
    "            logger: Logger instance.\n",
    "            optimize_sys_prompt: Prompt for optimization.\n",
    "            encoder: Embedding model.\n",
    "            r_s: List of reasoning paths.\n",
    "            D: Demonstration data.\n",
    "            enable_logger_rs: Log intermediate results.\n",
    "            K: Number of clusters for KMeans.\n",
    "            T: Number of optimizations per cluster.\n",
    "    \n",
    "        Returns:\n",
    "            final_answer: The aggregated answer.\n",
    "            overall_probability: Proportion of clusters supporting the final answer.\n",
    "        \"\"\"\n",
    "        raw_rs_embedding = []\n",
    "        for rs in r_s:\n",
    "            try:\n",
    "                embd = cls.create_embedding(rs, encoder)\n",
    "                if embd is not None and len(embd) > 0:\n",
    "                    raw_rs_embedding.append((rs, embd))\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"Error generating embedding: {str(e)}\")\n",
    "        if not raw_rs_embedding or len(raw_rs_embedding) < K:\n",
    "            logger.exception(\"Not enough valid reasoning paths for clustering.\")\n",
    "            return None, 0.0\n",
    "        raw_rs, raw_embeddings = zip(*raw_rs_embedding)\n",
    "        raw_rs = list(raw_rs)\n",
    "        raw_rs_embedding = np.array(raw_embeddings)\n",
    "    \n",
    "        # Step 2: Clustering\n",
    "        from sklearn.cluster import KMeans\n",
    "        kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(raw_rs_embedding)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "    \n",
    "        # Step 3: For each cluster, get best answer by majority (no weighting)\n",
    "        all_cluster_results = []\n",
    "        for cluster_idx in range(K):\n",
    "            indices = np.where(cluster_labels == cluster_idx)[0]\n",
    "            if len(indices) == 0:\n",
    "                continue\n",
    "            cluster_embeddings = raw_rs_embedding[indices]\n",
    "            distances = np.linalg.norm(cluster_embeddings - cluster_centers[cluster_idx], axis=1)\n",
    "            rep_index = indices[np.argmin(distances)]\n",
    "            rep_text = raw_rs[rep_index]\n",
    "            rk_emb = cls.create_embedding(rep_text, encoder)\n",
    "    \n",
    "            demos_scores = []\n",
    "            for d_j in D:\n",
    "                r_wrong_rs = d_j['wrong_rs']\n",
    "                r_wrong_embed = cls.create_embedding(r_wrong_rs, encoder)\n",
    "                sim = 0\n",
    "                if r_wrong_embed is not None:\n",
    "                    sim = np.dot(rk_emb, r_wrong_embed) / (np.linalg.norm(rk_emb) * np.linalg.norm(r_wrong_embed))\n",
    "                demos_scores.append((d_j, sim))\n",
    "            sorted_demos = sorted(demos_scores, key=lambda x: x[1], reverse=True)\n",
    "            top_demos = [i[0] for i in sorted_demos[:3]]\n",
    "            p_iter_rk = cls.construct_interven_prompt(\n",
    "                selected_demos=top_demos,\n",
    "                r_k=rep_text,\n",
    "                test_question=question,\n",
    "                test_context=context,\n",
    "                has_context=bool(context)\n",
    "            )\n",
    "            answers = []\n",
    "            for _ in range(T):\n",
    "                try:\n",
    "                    improved_rs, final_ans = cls.optimize_rs(p_iter_rk, optimize_sys_prompt, logger)\n",
    "                    answers.append(final_ans)\n",
    "                except Exception as e:\n",
    "                    logger.exception(f\"Error optimizing CoT for cluster {cluster_idx + 1}: {str(e)}\")\n",
    "            if answers:\n",
    "                best_answer, _ = Counter(answers).most_common(1)[0]\n",
    "                all_cluster_results.append(best_answer)\n",
    "    \n",
    "        if not all_cluster_results:\n",
    "            logger.exception(\"No valid cluster results\")\n",
    "            return None, 0.0\n",
    "        agg_counts = Counter(all_cluster_results)\n",
    "        final_answer, vote_count = agg_counts.most_common(1)[0]\n",
    "        overall_probability = vote_count / len(all_cluster_results)\n",
    "        logger.info(f\"Aggregated candidate votes (majority, with kmeans, no weight): {agg_counts}\")\n",
    "        return final_answer, overall_probability\n",
    "    \n",
    "    @classmethod\n",
    "    def causal_infer_with_fdr_no_kmeans_cluster(\n",
    "        cls,\n",
    "        question: str,\n",
    "        context: Any,\n",
    "        logger,\n",
    "        optimize_sys_prompt: str,\n",
    "        encoder,\n",
    "        r_s: List[str],\n",
    "        D: List[dict],\n",
    "        enable_logger_rs: bool = True,\n",
    "        T: int = 3,\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Ablation: No KMeans clustering, weighted aggregation.\n",
    "    \n",
    "        This variant **does not cluster** the reasoning paths. Instead, it treats each reasoning path independently.\n",
    "        For each path, T optimizations are performed, and the majority answer and its frequency (as a probability) are recorded.\n",
    "    \n",
    "        In final aggregation, each path's best answer contributes to the overall answer **weighted by its answer frequency**.\n",
    "        The answer with the highest summed weight across all paths is returned as the final answer, and its normalized weight\n",
    "        is reported as the estimated probability.\n",
    "    \n",
    "        Args:\n",
    "            question: The target question.\n",
    "            context: Additional context for the question.\n",
    "            logger: Logger instance.\n",
    "            optimize_sys_prompt: Prompt for optimization.\n",
    "            encoder: Embedding model.\n",
    "            r_s: List of reasoning paths.\n",
    "            D: Demonstration data.\n",
    "            enable_logger_rs: Log intermediate results.\n",
    "            T: Number of optimizations per reasoning path.\n",
    "    \n",
    "        Returns:\n",
    "            final_answer: The aggregated answer.\n",
    "            overall_probability: Weighted probability of the final answer.\n",
    "        \"\"\"\n",
    "        \n",
    "        raw_rs_embedding = []\n",
    "        for rs in r_s:\n",
    "            try:\n",
    "                embd = cls.create_embedding(rs, encoder)\n",
    "                if embd is not None and len(embd) > 0:\n",
    "                    raw_rs_embedding.append((rs, embd))\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"Error generating embedding: {str(e)}\")\n",
    "        if not raw_rs_embedding:\n",
    "            logger.exception(\"No valid reasoning paths.\")\n",
    "            return None, 0.0\n",
    "    \n",
    "        all_path_results = []\n",
    "        for idx, (rep_text, rk_emb) in enumerate(raw_rs_embedding):\n",
    "            demos_scores = []\n",
    "            for d_j in D:\n",
    "                r_wrong_rs = d_j['wrong_rs']\n",
    "                r_wrong_embed = cls.create_embedding(r_wrong_rs, encoder)\n",
    "                sim = 0\n",
    "                if r_wrong_embed is not None:\n",
    "                    sim = np.dot(rk_emb, r_wrong_embed) / (np.linalg.norm(rk_emb) * np.linalg.norm(r_wrong_embed))\n",
    "                demos_scores.append((d_j, sim))\n",
    "            sorted_demos = sorted(demos_scores, key=lambda x: x[1], reverse=True)\n",
    "            top_demos = [i[0] for i in sorted_demos[:3]]\n",
    "            p_iter_rk = cls.construct_interven_prompt(\n",
    "                selected_demos=top_demos,\n",
    "                r_k=rep_text,\n",
    "                test_question=question,\n",
    "                test_context=context,\n",
    "                has_context=bool(context)\n",
    "            )\n",
    "            answers = []\n",
    "            for _ in range(T):\n",
    "                try:\n",
    "                    improved_rs, final_ans = cls.optimize_rs(p_iter_rk, optimize_sys_prompt, logger)\n",
    "                    answers.append(final_ans)\n",
    "                except Exception as e:\n",
    "                    logger.exception(f\"Error optimizing CoT for reasoning path {idx + 1}: {str(e)}\")\n",
    "            answer_counts = Counter(answers)\n",
    "            if answer_counts:\n",
    "                best_answer, best_answer_count = answer_counts.most_common(1)[0]\n",
    "                prob_a_rk = best_answer_count / T\n",
    "                all_path_results.append({'final_answer': best_answer, 'prob_a_rk': prob_a_rk})\n",
    "    \n",
    "        # Weighted aggregation\n",
    "        aggregated_votes = {}\n",
    "        total_weight = 0.0\n",
    "        for res in all_path_results:\n",
    "            weight = res['prob_a_rk']\n",
    "            candidate = res['final_answer']\n",
    "            aggregated_votes[candidate] = aggregated_votes.get(candidate, 0) + weight\n",
    "            total_weight += weight\n",
    "        if not aggregated_votes:\n",
    "            logger.exception(\"No aggregated votes generated\")\n",
    "            return None, 0.0\n",
    "        final_answer, final_weight = max(aggregated_votes.items(), key=lambda x: x[1])\n",
    "        overall_probability = final_weight / total_weight if total_weight > 0 else 0\n",
    "        logger.info(f\"Aggregated candidate votes (weighted, no kmeans): {aggregated_votes}\")\n",
    "        return final_answer, overall_probability\n",
    "\n",
    "\n",
    "    @classmethod\n",
    "    async def async_optimize_rs(cls, prompt: str, optimized_sys_prompt: str, logger: Logger, max_tokens: int = 512, max_concurrent: int = 50) -> Tuple[str, str]:\n",
    "        try:\n",
    "            messages = [\n",
    "                {\"role\": \"system\", \"content\": optimized_sys_prompt},\n",
    "                {\"role\": \"user\", \"content\": prompt},\n",
    "            ]\n",
    "            results = await Doraemon.async_inference(\n",
    "                logger=logger,\n",
    "                prompts=[messages],\n",
    "                temperatures=[1.0],\n",
    "                max_tokens=max_tokens,\n",
    "                max_concurrent=max_concurrent\n",
    "            )\n",
    "            improved_rs, tokens = results[0]\n",
    "            answer_match = cls.get_answer(improved_rs)\n",
    "        except Exception as e:\n",
    "            logger.exception(f\"Error while querying LLM: {e}\")\n",
    "            improved_rs = None\n",
    "            answer_match = None\n",
    "        return improved_rs, answer_match\n",
    "    \n",
    "    @classmethod\n",
    "    async def async_create_embeddings(cls, raw_rs: List[str], encoder: SentenceTransformer, logger: Logger) -> np.ndarray:\n",
    "        raw_rs_embedding = []\n",
    "        for rs in raw_rs:\n",
    "            try:\n",
    "                embd = await cls.async_create_embedding(rs, encoder)  # Async version here\n",
    "                if embd is None or len(embd) == 0:\n",
    "                    logger.warning(f\"Empty or None embedding for reasoning path.\")\n",
    "                    continue\n",
    "                raw_rs_embedding.append(embd)\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"Error generating embedding: {str(e)}\")\n",
    "                raw_rs_embedding.append(None)\n",
    "        return np.array([emb for emb in raw_rs_embedding if emb is not None])\n",
    "    \n",
    "    @classmethod\n",
    "    async def async_create_embedding(cls, rs: str, encoder: SentenceTransformer) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Perform async embedding generation.\n",
    "        \"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        func = partial(encoder.encode, rs, show_progress_bar=False)\n",
    "        embd = await loop.run_in_executor(None, func)\n",
    "        return embd\n",
    "\n",
    "    \n",
    "    @classmethod\n",
    "    async def async_cluster_embeddings(\n",
    "        cls,\n",
    "        raw_rs_embedding: np.ndarray,\n",
    "        K: int,\n",
    "        logger: Logger\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Perform asynchronous clustering on the embeddings.\n",
    "        \"\"\"\n",
    "        loop = asyncio.get_event_loop()\n",
    "        cluster_labels, cluster_centers = await loop.run_in_executor(\n",
    "            None, \n",
    "            lambda: cls._perform_kmeans_clustering(raw_rs_embedding, K, logger)\n",
    "        )\n",
    "        return cluster_labels, cluster_centers\n",
    "    \n",
    "    @classmethod\n",
    "    def _perform_kmeans_clustering(\n",
    "        cls,\n",
    "        raw_rs_embedding: np.ndarray,\n",
    "        K: int,\n",
    "        logger: Logger\n",
    "    ) -> Tuple[np.ndarray, np.ndarray]:\n",
    "        \"\"\"\n",
    "        Helper method to perform KMeans clustering synchronously (called from an executor).\n",
    "        \"\"\"\n",
    "        kmeans = KMeans(n_clusters=K, random_state=42, n_init=10)\n",
    "        cluster_labels = kmeans.fit_predict(raw_rs_embedding)\n",
    "        cluster_centers = kmeans.cluster_centers_\n",
    "        return cluster_labels, cluster_centers\n",
    "    \n",
    "    @classmethod\n",
    "    async def async_process_clusters(\n",
    "        cls,\n",
    "        cluster_labels: np.ndarray,\n",
    "        cluster_centers: np.ndarray,\n",
    "        r_s: List[str],\n",
    "        D: List[Dict],\n",
    "        question: str,\n",
    "        context: Any,\n",
    "        logger: Logger,\n",
    "        K: int,\n",
    "        T: int,\n",
    "        optimize_sys_prompt: str,\n",
    "        enable_logger_rs: bool,\n",
    "        nwgm_mode: Optional[str],\n",
    "        max_tokens: int,\n",
    "        raw_rs_embedding: np.ndarray,\n",
    "        encoder,\n",
    "        max_concurrent: int = 50\n",
    "    ) -> List[Dict]:\n",
    "        all_cluster_results = []\n",
    "    \n",
    "        # Iterate over clusters and process each cluster asynchronously\n",
    "        for cluster_idx in range(K):\n",
    "            indices = np.where(cluster_labels == cluster_idx)[0]\n",
    "            if len(indices) == 0:\n",
    "                logger.info(f\"Cluster {cluster_idx + 1}: No items found\")\n",
    "                continue\n",
    "            prob_rk_input = len(indices) / raw_rs_embedding.shape[0]\n",
    "    \n",
    "            # Find the representative reasoning path (closest to cluster center)\n",
    "            cluster_embeddings = raw_rs_embedding[indices]\n",
    "            distances = np.linalg.norm(cluster_embeddings - cluster_centers[cluster_idx], axis=1)\n",
    "            rep_index = indices[np.argmin(distances)]\n",
    "            rep_text = r_s[rep_index]\n",
    "            rk_emb = raw_rs_embedding[rep_index]\n",
    "\n",
    "            demos_scores = []\n",
    "            for d_j in D:\n",
    "                r_wrong_rs = d_j['wrong_rs']\n",
    "                r_wrong_embed = await cls.async_create_embedding(r_wrong_rs, encoder)\n",
    "                sim = 0\n",
    "                if r_wrong_embed is not None:\n",
    "                    sim = np.dot(rk_emb, r_wrong_embed) / (np.linalg.norm(rk_emb) * np.linalg.norm(r_wrong_embed))\n",
    "                demos_scores.append((d_j, sim))\n",
    "            sorted_demos = sorted(demos_scores, key=lambda x: x[1], reverse=True)\n",
    "            top_demos = [i[0] for i in sorted_demos[:3]]\n",
    "    \n",
    "            # Construct prompt using selected demos\n",
    "            p_iter_rk = cls.construct_interven_prompt(\n",
    "                selected_demos=top_demos,\n",
    "                r_k=rep_text,\n",
    "                test_question=question,\n",
    "                test_context=context,\n",
    "                has_context=bool(context)\n",
    "            )\n",
    "\n",
    "            # Optimize reasoning paths T times (async!)\n",
    "            tasks = [cls.async_optimize_rs(p_iter_rk, optimize_sys_prompt, logger, max_tokens, max_concurrent) for _ in range(T)]\n",
    "            improved_results = await asyncio.gather(*tasks)\n",
    "            improved_rss = [improved_rs for improved_rs, _ in improved_results]\n",
    "            answers = [final_ans for _, final_ans in improved_results]\n",
    "    \n",
    "            # Determine best answer\n",
    "            answer_counts = Counter(answers)\n",
    "            if answer_counts:\n",
    "                best_answer, best_answer_count = answer_counts.most_common(1)[0]\n",
    "                prob_a_rk = best_answer_count / T\n",
    "            else:\n",
    "                best_answer = None\n",
    "                prob_a_rk = 0.0\n",
    "    \n",
    "            # Append the full cluster result\n",
    "            all_cluster_results.append({\n",
    "                'r_k': rep_text,\n",
    "                'answers': answers,\n",
    "                'final_answer': best_answer,\n",
    "                'prob_rk_input': prob_rk_input,\n",
    "                'prob_a_rk': prob_a_rk\n",
    "            })\n",
    "    \n",
    "        return all_cluster_results\n",
    "    \n",
    "    @classmethod\n",
    "    async def async_aggregate_votes(cls, all_cluster_results: List[Dict], logger: Logger) -> Tuple[str, float]:\n",
    "        # Calculate final answer and probabilities\n",
    "        aggregated_votes = {}\n",
    "        candidate_weights = {}\n",
    "        for res in all_cluster_results:\n",
    "            weight = res['prob_rk_input'] * res['prob_a_rk']\n",
    "            candidate = res['final_answer']\n",
    "            aggregated_votes[candidate] = aggregated_votes.get(candidate, 0) + weight\n",
    "            candidate_weights[candidate] = candidate_weights.get(candidate, 0) + weight\n",
    "        \n",
    "        if not aggregated_votes:\n",
    "            logger.exception(\"No aggregated votes generated\")\n",
    "            return None, 0.0\n",
    "        \n",
    "        # Select final answer\n",
    "        final_answer, final_weight = max(aggregated_votes.items(), key=lambda x: x[1])\n",
    "        overall_probability = final_weight / candidate_weights[final_answer] if candidate_weights[final_answer] > 0 else 0\n",
    "        \n",
    "        return final_answer, overall_probability\n",
    "\n",
    "    @classmethod\n",
    "    async def async_causal_infer_with_fdr(\n",
    "        cls,\n",
    "        question: str,\n",
    "        context: Any,\n",
    "        logger: Logger,\n",
    "        optimize_sys_prompt: str,\n",
    "        encoder,\n",
    "        r_s: List[str],\n",
    "        D: List[Dict],\n",
    "        enable_logger_rs: bool = True,\n",
    "        nwgm_mode: Optional[str] = None,\n",
    "        K: int = 4,\n",
    "        T: int = 3,\n",
    "        max_tokens: int = 512,\n",
    "        max_concurrent: int = 20\n",
    "    ) -> Tuple[str, float]:\n",
    "\n",
    "        # 1. Async create embeddings\n",
    "        raw_rs_embedding = await cls.async_create_embeddings(r_s, encoder, logger)\n",
    "        if len(raw_rs_embedding) < K:\n",
    "            logger.exception(f\"Not enough valid reasoning paths for clustering: {len(raw_rs_embedding)} found, {K} required. Skipping.\")\n",
    "            return None, 0.0\n",
    "\n",
    "        # 2. Async cluster\n",
    "        cluster_labels, cluster_centers = await cls.async_cluster_embeddings(raw_rs_embedding, K, logger)\n",
    "\n",
    "        # 3. Async process clusters\n",
    "        all_cluster_results = await cls.async_process_clusters(\n",
    "            cluster_labels, cluster_centers, r_s, D, question, context,\n",
    "            logger, K, T, optimize_sys_prompt, enable_logger_rs, nwgm_mode, max_tokens, raw_rs_embedding, encoder, max_concurrent\n",
    "        )\n",
    "\n",
    "        # 4. Aggregate\n",
    "        return await cls.async_aggregate_votes(all_cluster_results, logger)\n",
    "\n",
    "    @classmethod\n",
    "    async def async_causal_infer_with_fdr_kmeans_wo_weight(\n",
    "        cls,\n",
    "        question: str,\n",
    "        context: Any,\n",
    "        logger,\n",
    "        optimize_sys_prompt: str,\n",
    "        encoder,\n",
    "        r_s: List[str],\n",
    "        D: List[dict],\n",
    "        enable_logger_rs: bool = True,\n",
    "        K: int = 4,\n",
    "        T: int = 3,\n",
    "        max_tokens: int = 512,\n",
    "        max_concurrent: int = 20\n",
    "    ) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Async: KMeans clustering, no weighted aggregation.\n",
    "        \"\"\"\n",
    "        # 1. Async create embeddings\n",
    "        raw_rs_embedding = await cls.async_create_embeddings(r_s, encoder, logger)\n",
    "        if len(raw_rs_embedding) < K:\n",
    "            logger.exception(f\"Not enough valid reasoning paths for clustering: {len(raw_rs_embedding)} found, {K} required. Skipping.\")\n",
    "            return None, 0.0\n",
    "    \n",
    "        # 2. Async cluster\n",
    "        cluster_labels, cluster_centers = await cls.async_cluster_embeddings(raw_rs_embedding, K, logger)\n",
    "    \n",
    "        # 3. Async process clusters (reuse async_process_clusters, or create a variant if majority voting per-cluster is needed)\n",
    "        all_cluster_results = await cls.async_process_clusters(\n",
    "            cluster_labels, cluster_centers, r_s, D, question, context,\n",
    "            logger, K, T, optimize_sys_prompt, enable_logger_rs, None, max_tokens, raw_rs_embedding, encoder, max_concurrent\n",
    "        )\n",
    "    \n",
    "        # 4. Aggregate with majority vote, not weighted\n",
    "        return await cls.async_aggregate_votes_majority(all_cluster_results, logger)\n",
    "    \n",
    "    @classmethod\n",
    "    async def async_aggregate_votes_majority(cls, all_cluster_results: List[dict], logger) -> Tuple[str, float]:\n",
    "        \"\"\"\n",
    "        Aggregates answers by simple majority vote (not weighted).\n",
    "        \"\"\"\n",
    "        candidate_list = [res['final_answer'] for res in all_cluster_results if res['final_answer'] is not None]\n",
    "        if not candidate_list:\n",
    "            logger.exception(\"No valid cluster results for majority vote\")\n",
    "            return None, 0.0\n",
    "    \n",
    "        agg_counts = Counter(candidate_list)\n",
    "        final_answer, vote_count = agg_counts.most_common(1)[0]\n",
    "        overall_probability = vote_count / len(candidate_list)\n",
    "        logger.info(f\"Aggregated candidate votes (majority, with kmeans, no weight): {agg_counts}\")\n",
    "        return final_answer, overall_probability\n",
    "\n",
    "    @classmethod\n",
    "    async def async_causal_infer_with_fdr_no_kmeans_cluster(\n",
    "        cls,\n",
    "        question: str,\n",
    "        context: Any,\n",
    "        logger,\n",
    "        optimize_sys_prompt: str,\n",
    "        encoder,\n",
    "        r_s: List[str],\n",
    "        D: List[dict],\n",
    "        enable_logger_rs: bool = True,\n",
    "        T: int = 3,\n",
    "        max_tokens: int = 512,\n",
    "    ) -> Tuple[str, float]:\n",
    "        # 1. Async create embeddings\n",
    "        raw_rs_embedding = await cls.async_create_embeddings(r_s, encoder, logger)\n",
    "        if len(raw_rs_embedding) == 0:\n",
    "            logger.exception(\"No valid reasoning paths.\")\n",
    "            return None, 0.0\n",
    "    \n",
    "        # 2. Async process paths (per path, no clustering)\n",
    "        all_path_results = await cls.async_process_paths(\n",
    "            raw_rs_embedding, r_s, D, question, context,\n",
    "            logger, encoder, T, optimize_sys_prompt, enable_logger_rs, max_tokens\n",
    "        )\n",
    "    \n",
    "        # 3. Weighted aggregation (reuse async_aggregate_votes)\n",
    "        return await cls.async_aggregate_votes(all_path_results, logger)\n",
    "    \n",
    "    \n",
    "    @classmethod\n",
    "    async def async_process_paths(\n",
    "        cls,\n",
    "        raw_rs_embedding: np.ndarray,\n",
    "        r_s: List[str],\n",
    "        D: List[dict],\n",
    "        question: str,\n",
    "        context: Any,\n",
    "        logger,\n",
    "        encoder,\n",
    "        T: int,\n",
    "        optimize_sys_prompt: str,\n",
    "        enable_logger_rs: bool,\n",
    "        max_tokens: int,\n",
    "        max_concurrent: int = 50\n",
    "    ) -> List[dict]:\n",
    "        \"\"\"\n",
    "        For each reasoning path: select top demos, optimize T times (async), get best answer and its probability.\n",
    "        Return a list of dicts like: {'final_answer': ..., 'prob_a_rk': ...}\n",
    "        \"\"\"\n",
    "        all_path_results = []\n",
    "        n_paths = len(r_s)\n",
    "        for idx, (rep_text, rk_emb) in enumerate(zip(r_s, raw_rs_embedding)):\n",
    "            demos_scores = []\n",
    "            for d_j in D:\n",
    "                r_wrong_rs = d_j['wrong_rs']\n",
    "                r_wrong_embed = await cls.async_create_embedding(r_wrong_rs, encoder)\n",
    "                sim = 0\n",
    "                if r_wrong_embed is not None:\n",
    "                    sim = np.dot(rk_emb, r_wrong_embed) / (np.linalg.norm(rk_emb) * np.linalg.norm(r_wrong_embed))\n",
    "                demos_scores.append((d_j, sim))\n",
    "            sorted_demos = sorted(demos_scores, key=lambda x: x[1], reverse=True)\n",
    "            top_demos = [i[0] for i in sorted_demos[:3]]\n",
    "            p_iter_rk = cls.construct_interven_prompt(\n",
    "                selected_demos=top_demos,\n",
    "                r_k=rep_text,\n",
    "                test_question=question,\n",
    "                test_context=context,\n",
    "                has_context=bool(context)\n",
    "            )\n",
    "            tasks = [cls.async_optimize_rs(p_iter_rk, optimize_sys_prompt, logger, max_tokens, max_concurrent) for _ in range(T)]\n",
    "            improved_results = await asyncio.gather(*tasks)\n",
    "            answers = [final_ans for _, final_ans in improved_results if final_ans is not None]\n",
    "            answer_counts = Counter(answers)\n",
    "            if answer_counts:\n",
    "                best_answer, best_answer_count = answer_counts.most_common(1)[0]\n",
    "                prob_a_rk = best_answer_count / T\n",
    "                all_path_results.append({'final_answer': best_answer, 'prob_a_rk': prob_a_rk, 'prob_rk_input': 1.0 / n_paths})\n",
    "    \n",
    "        return all_path_results\n",
    "\n",
    "    @classmethod\n",
    "    async def async_evaluate(\n",
    "        cls,\n",
    "        tasks: List[Dict[str, Any]],\n",
    "        logger,\n",
    "        op_system_prompt: str,\n",
    "        encoder,\n",
    "        D,\n",
    "        enable_logger_rs=True,\n",
    "        ablation: str = \"all\",\n",
    "        K: int = 4,\n",
    "        T: int = 3,\n",
    "        max_tokens: int = 512,\n",
    "        max_concurrent: int = 20,\n",
    "        p_b: bool = True\n",
    "    ) -> pd.DataFrame:\n",
    "    \n",
    "        async def evaluate_single_row(row):\n",
    "            try:\n",
    "                q = row.get('query')\n",
    "                r_p = row.get('r_s')\n",
    "                context = row.get('context') if row.get('context') else None\n",
    "\n",
    "                if ablation in ['all', 'reverse', 'random']:\n",
    "                    final_answer, o_p = await cls.async_causal_infer_with_fdr(\n",
    "                        q, context, logger, op_system_prompt, encoder, r_p, D, enable_logger_rs, ablation, K, T, max_tokens, max_concurrent\n",
    "                    )\n",
    "                elif ablation == 'wo_km':\n",
    "                    final_answer, o_p = await cls.async_causal_infer_with_fdr_no_kmeans_cluster(\n",
    "                        q, context, logger, op_system_prompt, encoder, r_p, D, enable_logger_rs\n",
    "                    )\n",
    "                elif ablation == 'wo_wt':\n",
    "                    final_answer, o_p = await cls.async_causal_infer_with_fdr_kmeans_wo_weight(\n",
    "                        q, context, logger, op_system_prompt, encoder, r_p, D, enable_logger_rs, K, T, max_tokens, max_concurrent\n",
    "                    )\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown mode: {ablation}\")\n",
    "    \n",
    "                if final_answer is not None:\n",
    "                    try:\n",
    "                        final_answer_clean = cls.get_answer(final_answer)\n",
    "                    except Exception as e:\n",
    "                        logger.exception(f\"Exception in get_answer for question: {q}, error: {e}\")\n",
    "                        final_answer_clean = None\n",
    "                else:\n",
    "                    final_answer_clean = None\n",
    "    \n",
    "                row = row.copy()\n",
    "                row['final_answer'] = final_answer_clean\n",
    "                row['o_p'] = o_p\n",
    "                return row\n",
    "            except Exception as e:\n",
    "                logger.exception(f\"Exception in evaluate_row for question: {row.get('question')}, error: {e}\")\n",
    "                row = row.copy()\n",
    "                row['final_answer'] = None\n",
    "                row['o_p'] = None\n",
    "                return row\n",
    "\n",
    "        # Build all coroutines\n",
    "        all_coros = [evaluate_single_row(row) for row in tasks]\n",
    "\n",
    "        if p_b:\n",
    "            results = await tqdm_asyncio.gather(\n",
    "                *all_coros,\n",
    "                total=len(all_coros),\n",
    "                desc=\"Evaluating rows\"\n",
    "            )\n",
    "        else:\n",
    "            # 📊 Use tqdm_asyncio.gather for a high-level progress bar\n",
    "            results = await asyncio.gather(*all_coros)\n",
    "        \n",
    "        filtered_results = [r for r in results if r is not None]\n",
    "        return pd.DataFrame(filtered_results)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "sourceId": 250973825,
     "sourceType": "kernelVersion"
    }
   ],
   "dockerImageVersionId": 31040,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 61.241794,
   "end_time": "2025-07-25T07:03:07.468324",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-07-25T07:02:06.226530",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
